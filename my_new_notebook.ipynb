{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUahfk1q2tnYhFZkF/WaOI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadia-github2908/Project-01/blob/main/my_new_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THINGS TO TRY ANSWERS OF MODULE 4-LAB 1- PERCEPTRONS**"
      ],
      "metadata": {
        "id": "xysYoVxE4PL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) Try this on a real dataset like the Sonar dataset or the Banknote Dataset and show the error plot..\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Ans 1) let's use the Sonar dataset from the UCI Machine Learning Repository. This dataset consists of sonar signals bounced off a metal cylinder and a cylindrical rock. The task is to classify the object as either a rock or a metal cylinder.\n",
        "\n",
        "Load the Dataset:\n",
        "You need to download the dataset and load it into your Python environment. You can use the pandas library to load the dataset. Here's a simple example:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HRIaYrXP4ecZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Download the Sonar dataset from UCI ML Repository\n",
        "sonar_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
        "sonar_df = pd.read_csv(sonar_url, header=None)\n"
      ],
      "metadata": {
        "id": "pdT2S9zV5VLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the Data:\n",
        "The Sonar dataset has two classes labeled as 'R' (Rock) and 'M' (Metal). You need to convert these labels to numeric values, such as -1 and 1, which are suitable for the perceptron algorithm."
      ],
      "metadata": {
        "id": "aszuom0y5Zx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert labels to numeric values (-1 and 1)\n",
        "sonar_df[60] = sonar_df[60].map({'R': -1, 'M': 1})\n"
      ],
      "metadata": {
        "id": "7-fiToXP5bXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the Data:\n",
        "Split the dataset into features (X) and labels (y). Then, split it into training and testing sets"
      ],
      "metadata": {
        "id": "sZnr2HQ05eBj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = sonar_df.iloc[:, :-1].values\n",
        "y = sonar_df.iloc[:, -1].values\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "DJaQ-3QT5k21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply Perceptron Algorithm:\n",
        "Use the perceptron algorithm function you defined earlier on the training data."
      ],
      "metadata": {
        "id": "1sJH6qQj5lht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = perceptron_algo(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "NaFAjM2_5pr3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate and Plot Errors:\n",
        "Evaluate the accuracy of the perceptron algorithm on the test set and plot the errors over epochs."
      ],
      "metadata": {
        "id": "XHE5e12n5r7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perceptron_algo_with_errors(X, Y):\n",
        "    w = np.zeros(len(X[0]))\n",
        "    eta = 1\n",
        "    epochs = 10\n",
        "    errors = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        error_count = 0\n",
        "        for i, x in enumerate(X):\n",
        "            if (np.dot(X[i], w) * Y[i]) <= 0:\n",
        "                w = w + eta * X[i] * Y[i]\n",
        "                error_count += 1\n",
        "        errors.append(error_count)\n",
        "\n",
        "    return w, errors\n",
        "\n",
        "w, errors = perceptron_algo_with_errors(X_train, y_train)\n",
        "\n",
        "plt.plot(errors)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Number of Errors')\n",
        "plt.title('Perceptron Algorithm - Training Errors Over Epochs')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "foqFxs4f5wJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a modified perceptron algorithm function (perceptron_algo_with_errors) that returns the learned weights and a list of errors for each epoch. The errors are then plotted over epochs.\n",
        "\n",
        "The plot will show you how the number of errors changes as the algorithm iterates through the training data. You may need to adjust parameters like the learning rate (eta) and the number of epochs based on the convergence behavior."
      ],
      "metadata": {
        "id": "TvLXC1TT6JH3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)Increase/decrease the learning rate to see how many iterations will be take to coverge. Does it even converge on a huge learning rate?\n",
        "\n",
        "\n",
        "Ans 2)Let's modify the code to experiment with different learning rates and observe how it affects the convergence of the perceptron algorithm. We will create a function to visualize the number of errors over epochs for different learning rates:"
      ],
      "metadata": {
        "id": "M4lbOlTD6Lhj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def perceptron_algo_with_errors(X, Y, eta):\n",
        "    w = np.zeros(len(X[0]))\n",
        "    epochs = 10\n",
        "    errors = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        error_count = 0\n",
        "        for i, x in enumerate(X):\n",
        "            if (np.dot(X[i], w) * Y[i]) <= 0:\n",
        "                w = w + eta * X[i] * Y[i]\n",
        "                error_count += 1\n",
        "        errors.append(error_count)\n",
        "\n",
        "    return w, errors\n",
        "\n",
        "def visualize_errors(X_train, y_train, learning_rates):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    for eta in learning_rates:\n",
        "        w, errors = perceptron_algo_with_errors(X_train, y_train, eta)\n",
        "        plt.plot(errors, label=f'Learning Rate: {eta}')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Number of Errors')\n",
        "    plt.title('Perceptron Algorithm - Training Errors Over Epochs')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Download the Sonar dataset\n",
        "sonar_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data\"\n",
        "sonar_df = pd.read_csv(sonar_url, header=None)\n",
        "\n",
        "# Convert labels to numeric values (-1 and 1)\n",
        "sonar_df[60] = sonar_df[60].map({'R': -1, 'M': 1})\n",
        "\n",
        "# Split the dataset\n",
        "X = sonar_df.iloc[:, :-1].values\n",
        "y = sonar_df.iloc[:, -1].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Experiment with different learning rates\n",
        "learning_rates = [0.1, 0.5, 1.0, 1.5]\n",
        "visualize_errors(X_train, y_train, learning_rates)\n"
      ],
      "metadata": {
        "id": "PgnOJ4OF6tJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a function visualize_errors that runs the perceptron algorithm with different learning rates and plots the number of errors over epochs for each learning rate. It is noted that a very high learning rate might prevent convergence, causing the algorithm to oscillate or diverge. On the other hand, a very low learning rate might result in slow convergence."
      ],
      "metadata": {
        "id": "ZbVNJxBD68I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)Take a toy dataset that is not linearly separable and run the perceptron algorithm on it. What happens? Note your observations. An example is given below:\n",
        "\n",
        "\n",
        "Ans 3)Let's create a toy dataset that is not linearly separable and observe the behavior of the perceptron algorithm. We'll use the 'make_classification' function from scikit-learn to generate a synthetic dataset with two classes:"
      ],
      "metadata": {
        "id": "FEwm6PUm7MJQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a toy dataset that is not linearly separable\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0,\n",
        "                           n_clusters_per_class=1, class_sep=0.5, random_state=42)\n",
        "\n",
        "# Map class labels to -1 and 1\n",
        "y = np.where(y == 0, -1, 1)\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Visualize the toy dataset\n",
        "plt.scatter(X_train[y_train == -1][:, 0], X_train[y_train == -1][:, 1], marker='o', label='Class -1')\n",
        "plt.scatter(X_train[y_train == 1][:, 0], X_train[y_train == 1][:, 1], marker='+', label='Class 1')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Toy Dataset: Not Linearly Separable')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jaIJ_W2G7j4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates a synthetic dataset with two classes, where the classes are not linearly separable. The classes are visualized using different markers ('o' for Class -1 and '+' for Class 1). let's take and run a  perceptron algorithm on this dataset and observe what happens:"
      ],
      "metadata": {
        "id": "076FCkuy7k28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run perceptron algorithm on the toy dataset\n",
        "w, errors = perceptron_algo_with_errors(X_train, y_train, eta=0.1)\n",
        "\n",
        "# Visualize the number of errors over epochs\n",
        "plt.plot(errors)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Number of Errors')\n",
        "plt.title('Perceptron Algorithm - Training Errors Over Epochs')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Uq1sttJD70W3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code applies the perceptron algorithm to the toy dataset and plots the number of errors over epochs. Since the dataset is not linearly separable, the perceptron algorithm may not converge, and the number of errors might not decrease to zero."
      ],
      "metadata": {
        "id": "LuwKupZp73Hr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THINGS TO TRY ANSWERS OF MODULE 4-LAB 2- PERCEPTRONS**"
      ],
      "metadata": {
        "id": "BErKNxwb79jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Change the coefficients array and try a different polynomial instead of our x2.\n",
        "\n",
        "\n",
        "Ans 1)we will use  use the polynomial y=2x^2-3x+1"
      ],
      "metadata": {
        "id": "KCEn9nWC8Ib7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# ... (rest of the code remains unchanged)\n",
        "\n",
        "# Change the coefficients array to represent the new polynomial: y = 2x^2 - 3x + 1\n",
        "coeffs = [2, -3, 1]\n",
        "\n",
        "# ... (rest of the code remains unchanged)\n"
      ],
      "metadata": {
        "id": "AWxjjBfl-5tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " The rest of the code will then use this polynomial for data generation, jittering, initial model prediction, and gradient descent"
      ],
      "metadata": {
        "id": "dKFWZjOX_KIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)Increase/decrease the learning rate to see how many iterations will be take to coverge. Does it even converge on a huge learning rate?\n",
        "\n",
        "\n",
        "Ans 2)The learning rate is a crucial hyperparameter in gradient descent, affecting the convergence speed and stability. Setting it too high may cause the algorithm to oscillate or diverge, while setting it too low may slow down convergence. Let's experiment by adjusting the learning rate in the code:"
      ],
      "metadata": {
        "id": "BOoiL3FQ_MJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (rest of the code remains unchanged)\n",
        "\n",
        "# Experiment with different learning rates\n",
        "lr_values = [0.001, 0.01, 0.1, 1.0]\n",
        "\n",
        "for lr in lr_values:\n",
        "    GD = gradient_descent(30000, lr)\n",
        "    print(f\"Final Coefficients predicted with lr={lr}: {GD[1]}\")\n",
        "    print(f\"Original Coefficients: {coeffs}\")\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(GD[3], label=f'lr={lr}')\n",
        "    plt.title('Loss over iterations for different learning rates')\n",
        "    plt.legend(loc=\"upper right\")\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('MSE')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "EKX4Z-V__roO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe the behavior for each learning rate. Too large of a learning rate may result in divergence or oscillations in the loss, while too small of a learning rate may lead to slow convergence. It's important to strike a balance to achieve a good convergence speed without sacrificing stability. Adjust the learning rates in the 'lr_values' list and see how the algorithm behaves.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QCfUfJiP_sid"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)Take a degree 5 polynomial with 5 roots and try different initializations, instead of random ones. Does it converge on different values for different initializations? Why does initialization not matter in our case of  x2 ?"
      ],
      "metadata": {
        "id": "7n4pAHcI_4Nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 3) Let's modify the code to use a degree 5 polynomial with 5 roots and experiment with different initializations:"
      ],
      "metadata": {
        "id": "tul5Zsv7_7fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (rest of the code remains unchanged)\n",
        "\n",
        "# Change the degree 2 polynomial to a degree 5 polynomial with 5 roots\n",
        "def eval_5th_degree(coeffs, x):\n",
        "    \"\"\"\n",
        "    Function to return the output of evaluating a fifth degree polynomial,\n",
        "    given a specific x value.\n",
        "\n",
        "    Args:\n",
        "        coeffs: List containing the coefficients a, b, c, d, e, and f for the polynomial.\n",
        "        x: The input x value to the polynomial.\n",
        "\n",
        "    Returns:\n",
        "        y: The corresponding output y value for the fifth degree polynomial.\n",
        "\n",
        "    \"\"\"\n",
        "    a = coeffs[0] * (x ** 5)\n",
        "    b = coeffs[1] * (x ** 4)\n",
        "    c = coeffs[2] * (x ** 3)\n",
        "    d = coeffs[3] * (x ** 2)\n",
        "    e = coeffs[4] * x\n",
        "    f = coeffs[5]\n",
        "    y = a + b + c + d + e + f\n",
        "    return y\n",
        "\n",
        "# Set coefficients for a degree 5 polynomial with 5 roots\n",
        "coeffs_degree_5 = [1, -2, -1, 1, 2, 1]\n",
        "\n",
        "# ... (rest of the code remains unchanged)\n"
      ],
      "metadata": {
        "id": "9L6w1veyAPmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " you can use 'eval_5th_degree' instead of 'eval_2nd_degree' in your code, and set 'coeffs_degree_5' to a set of coefficients representing a degree 5 polynomial with 5 roots.\n"
      ],
      "metadata": {
        "id": "GhVBXAztARkZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4)Can you modify the algorithm to find a maxima of a function, instead of a minima?"
      ],
      "metadata": {
        "id": "qaHpHh2QAd1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 4)To modify the algorithm to find the maximum of a function, you can simply change the sign of the gradients during the parameter updates. In gradient ascent, you move in the direction of the positive gradient to maximize the function. Here's how you can modify the relevant part of the code:"
      ],
      "metadata": {
        "id": "DqsiAZKPAjY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (rest of the code remains unchanged)\n",
        "\n",
        "def calc_gradient_2nd_poly_for_GD(coeffs, inputs_x, outputs_y, lr):\n",
        "    \"\"\"\n",
        "    calculates the gradient for a second degree polynomial.\n",
        "\n",
        "    Args:\n",
        "        coeffs: a, b, and c, for a 2nd degree polynomial [ y = ax^2 + bx + c ]\n",
        "        inputs_x: x input datapoints\n",
        "        outputs_y: actual y output points\n",
        "        lr: learning rate\n",
        "\n",
        "    Returns: Calculated gradients for the 2nd degree polynomial, as a tuple of its parts for a, b, c respectively.\n",
        "\n",
        "    \"\"\"\n",
        "    a_s = []\n",
        "    b_s = []\n",
        "    c_s = []\n",
        "\n",
        "    y_bars = eval_2nd_degree(coeffs, inputs_x)\n",
        "\n",
        "    for x, y, y_bar in list(zip(inputs_x, outputs_y, y_bars)):\n",
        "        x_squared = x ** 2\n",
        "        partial_a = -x_squared * (y - y_bar)  # Change the sign here\n",
        "        a_s.append(partial_a)\n",
        "        partial_b = -x * (y - y_bar)  # Change the sign here\n",
        "        b_s.append(partial_b)\n",
        "        partial_c = -(y - y_bar)  # Change the sign here\n",
        "        c_s.append(partial_c)\n",
        "\n",
        "    num = [i for i in y_bars]\n",
        "    n = len(num)\n",
        "\n",
        "    gradient_a = (-2 / n) * sum(a_s)\n",
        "    gradient_b = (-2 / n) * sum(b_s)\n",
        "    gradient_c = (-2 / n) * sum(c_s)\n",
        "\n",
        "    a_new = coeffs[0] + lr * gradient_a  # Change the sign here\n",
        "    b_new = coeffs[1] + lr * gradient_b  # Change the sign here\n",
        "    c_new = coeffs[2] + lr * gradient_c  # Change the sign here\n",
        "\n",
        "    new_model_coeffs = (a_new, b_new, c_new)\n",
        "\n",
        "    new_y_bar = eval_2nd_degree(new_model_coeffs, inputs_x)\n",
        "\n",
        "    updated_model_loss = loss_mse(outputs_y, new_y_bar)\n",
        "    return updated_model_loss, new_model_coeffs, new_y_bar\n",
        "\n",
        "# ... (rest of the code remains unchanged)\n",
        "\n",
        "# Use gradient ascent instead of gradient descent\n",
        "def gradient_ascent(epochs, lr):\n",
        "    \"\"\"\n",
        "    Perform gradient ascent for a second degree polynomial.\n",
        "\n",
        "    Args:\n",
        "        epochs: number of iterations to perform finding new coefficients and updating loss.\n",
        "        lr: specified learning rate\n",
        "\n",
        "    Returns: Tuple containing (updated_model_loss, new_model_coeffs, new_y_bar predictions, saved loss updates)\n",
        "\n",
        "    \"\"\"\n",
        "    losses = []\n",
        "    rand_coeffs_to_test = rand_coeffs\n",
        "    for i in range(epochs):\n",
        "        loss = calc_gradient_2nd_poly_for_GD(rand_coeffs_to_test, hundred_xs, ys, lr)\n",
        "        rand_coeffs_to_test = loss[1]\n",
        "        losses.append(loss[0])\n",
        "    print(losses)\n",
        "    return loss[0], loss[1], loss[2], losses\n",
        "\n",
        "# ... (rest of the code remains unchanged)\n",
        "\n",
        "# Example of using gradient ascent\n",
        "GA = gradient_ascent(30000, 0.0003)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(xs, ys, 'g+', label='original')\n",
        "plt.plot(xs, GA[2], 'b.', label='final_prediction')\n",
        "plt.title('Original vs Final prediction after Gradient Ascent')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final Coefficients predicted with Gradient Ascent: {GA[1]}\")\n",
        "print(f\"Original Coefficients: {coeffs}\")\n"
      ],
      "metadata": {
        "id": "gbAc7DlJA4dS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the algorithm will attempt to find the maximum of the second-degree polynomial instead of the minimum.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "z-2FFjfPA8FS"
      }
    }
  ]
}