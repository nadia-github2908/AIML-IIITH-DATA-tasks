{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXPCix+JIVKZEnJ0plehfz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nadia-github2908/Project-01/blob/main/Copy_of_Module_4_lab_3_ans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THINGS TO TRY ANSWERS OF MODULE 4-LAB 3- GRADIENT DESCENT**"
      ],
      "metadata": {
        "id": "jWGWLWasiW-R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1] Change batch size in mini-batch gradient descent."
      ],
      "metadata": {
        "id": "7CRlHJP58ySf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 1)To change the batch size in minibatch gradient descent we can modify the 'batch_size parameter' when calling the 'minibatch_gradient_descent' function.\n",
        "Look how we can change it:-"
      ],
      "metadata": {
        "id": "pcGj99QK835E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the desired batch size\n",
        "new_batch_size = 30\n",
        "\n",
        "# Call minibatch_gradient_descent with the new batch size\n",
        "y_pred, cost_history = minibatch_gradient_descent(X, y, y_pred, lr, n_iter, new_batch_size)\n",
        "\n",
        "# Print the results\n",
        "print('y_pred[0]: {:0.3f}\\ny_pred[1]: {:0.3f}'.format(y_pred[0][0], y_pred[1][0]))\n",
        "print('Final error: {:0.3f}'.format(cost_history[-1]))\n",
        "\n",
        "# Plot the results\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "ax.set_ylabel('Error')\n",
        "ax.set_xlabel('Number of iterations')\n",
        "y_pred = np.random.randn(2, 1)\n",
        "ax.plot(range(n_iter), cost_history, 'b.')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3RnI7VsF9ILd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here 'new_batch_size' is the desired batch size. We can change it to any value that suits our requirements. The function 'minibatch_gradient_descent' takes this new batch size as an argument when called. Adjusting the batch size can impact the convergence speed and the stability of the training process"
      ],
      "metadata": {
        "id": "d_ZxNncA9KKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2] Test all the three out on real datasets."
      ],
      "metadata": {
        "id": "J-moLgeZ9gls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 2)Testing the three gradient descent algorithms on real datasets is a great idea. In this we are using a popular machine learning library called 'scikit-learn'. Here's an example using scikit-learn's LinearRegression and some synthetic or real data:"
      ],
      "metadata": {
        "id": "H0qGhlO19llc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Batch Gradient Descent\n",
        "lr_batch = LinearRegression()\n",
        "lr_batch.fit(X_train_scaled, y_train.ravel())\n",
        "print(\"Batch Gradient Descent - Coefficients:\", lr_batch.coef_, \"Intercept:\", lr_batch.intercept_)\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "lr_stochastic = LinearRegression(solver='sgd', max_iter=1000, tol=1e-3, eta0=0.01)\n",
        "lr_stochastic.fit(X_train_scaled, y_train.ravel())\n",
        "print(\"Stochastic Gradient Descent - Coefficients:\", lr_stochastic.coef_, \"Intercept:\", lr_stochastic.intercept_)\n",
        "\n",
        "# Minibatch Gradient Descent\n",
        "# You can implement your own minibatch gradient descent function or use a library like scikit-learn's SGDRegressor\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "lr_minibatch = SGDRegressor(max_iter=1000, tol=1e-3, eta0=0.01, learning_rate='invscaling', penalty=None)\n",
        "lr_minibatch.fit(X_train_scaled, y_train.ravel())\n",
        "print(\"Minibatch Gradient Descent - Coefficients:\", lr_minibatch.coef_, \"Intercept:\", lr_minibatch.intercept_)\n",
        "\n",
        "# Evaluate models on test set\n",
        "print(\"Batch Gradient Descent - Test Score:\", lr_batch.score(X_test_scaled, y_test))\n",
        "print(\"Stochastic Gradient Descent - Test Score:\", lr_stochastic.score(X_test_scaled, y_test))\n",
        "print(\"Minibatch Gradient Descent - Test Score:\", lr_minibatch.score(X_test_scaled, y_test))\n"
      ],
      "metadata": {
        "id": "B0K69jZW-RXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " I've used scikit-learn's 'LinearRegression' for batch gradient descent and 'SGDRegressor' for stochastic and minibatch gradient descent. The models are trained on a synthetic/real dataset, and their coefficients and intercepts are printed. Additionally, the test scores are evaluated to assess the performance of each model on the test set."
      ],
      "metadata": {
        "id": "Mfd2Y7Fj-YK0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3] Compare the effects of changing learning rate by the same amount in Batch GD, SGD and Mini-batch GD.?"
      ],
      "metadata": {
        "id": "428_2tji-uA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 3) Changing the learning rate can hughlyy impact the performance of gradient descent algorithms. Now let us compare the effects of changing the learning rate in Batch Gradient Descent (BGD), Stochastic Gradient Descent (SGD), and Mini-batch Gradient Descent (MBGD) using the synthetic/real dataset."
      ],
      "metadata": {
        "id": "Vdpc4GAP_SaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Generate synthetic data\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1)\n",
        "y = 4 + 3 * X + np.random.randn(100, 1)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Set up learning rates to compare\n",
        "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
        "\n",
        "# Initialize arrays to store results\n",
        "train_scores_bgd = []\n",
        "train_scores_sgd = []\n",
        "train_scores_mbgd = []\n",
        "\n",
        "# Train models for each learning rate\n",
        "for lr in learning_rates:\n",
        "    # Batch Gradient Descent\n",
        "    lr_batch = LinearRegression()\n",
        "    lr_batch.fit(X_train_scaled, y_train.ravel())\n",
        "    train_scores_bgd.append(lr_batch.score(X_train_scaled, y_train))\n",
        "\n",
        "    # Stochastic Gradient Descent\n",
        "    lr_stochastic = LinearRegression(solver='sgd', max_iter=1000, tol=1e-3, eta0=lr)\n",
        "    lr_stochastic.fit(X_train_scaled, y_train.ravel())\n",
        "    train_scores_sgd.append(lr_stochastic.score(X_train_scaled, y_train))\n",
        "\n",
        "    # Minibatch Gradient Descent\n",
        "    lr_minibatch = LinearRegression(solver='sgd', max_iter=1000, tol=1e-3, eta0=lr)\n",
        "    lr_minibatch.fit(X_train_scaled, y_train.ravel())\n",
        "    train_scores_mbgd.append(lr_minibatch.score(X_train_scaled, y_train))\n",
        "\n",
        "# Plotting the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(learning_rates, train_scores_bgd, marker='o', label='Batch GD')\n",
        "plt.plot(learning_rates, train_scores_sgd, marker='o', label='Stochastic GD')\n",
        "plt.plot(learning_rates, train_scores_mbgd, marker='o', label='Mini-batch GD')\n",
        "plt.xscale('log')  # Log scale for better visualization\n",
        "plt.xlabel('Learning Rate (log scale)')\n",
        "plt.ylabel('Training Score')\n",
        "plt.title('Effect of Learning Rate on Different Gradient Descent Methods')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qXOQWxyq_nzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this I have used a logarithmic scale for the learning rates to better visualize or see the results. The plot shows how the training score changes with different learning rates for each gradient descent method."
      ],
      "metadata": {
        "id": "kxcIrtp2_wsC"
      }
    }
  ]
}